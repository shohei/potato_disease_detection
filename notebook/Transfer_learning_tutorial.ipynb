{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a3e1b4f-c484-439a-b6f8-d7083298148e",
   "metadata": {},
   "source": [
    "# チュートリアル\n",
    "https://torch.classcat.com/category/transfer-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b705993a-06a8-49b1-a56c-de4b21536c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x116e7d6d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# License: BSD\n",
    "# Author: Sasank Chilamkurthy\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e66003f9-b2f0-41bf-a9f1-0b88e0967b40",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'potato/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 17\u001b[0m\n\u001b[1;32m      1\u001b[0m data_transforms \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m: transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m      3\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mRandomResizedCrop(\u001b[38;5;241m224\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     ]),\n\u001b[1;32m     14\u001b[0m }\n\u001b[1;32m     16\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpotato\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 17\u001b[0m image_datasets \u001b[38;5;241m=\u001b[39m {x: datasets\u001b[38;5;241m.\u001b[39mImageFolder(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, x),\n\u001b[1;32m     18\u001b[0m                                           data_transforms[x])\n\u001b[1;32m     19\u001b[0m                   \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[1;32m     20\u001b[0m dataloaders \u001b[38;5;241m=\u001b[39m {x: torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(image_datasets[x], batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m     21\u001b[0m                                              shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     22\u001b[0m               \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[1;32m     23\u001b[0m dataset_sizes \u001b[38;5;241m=\u001b[39m {x: \u001b[38;5;28mlen\u001b[39m(image_datasets[x]) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n",
      "Cell \u001b[0;32mIn[2], line 17\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m data_transforms \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m: transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m      3\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mRandomResizedCrop(\u001b[38;5;241m224\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     ]),\n\u001b[1;32m     14\u001b[0m }\n\u001b[1;32m     16\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpotato\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 17\u001b[0m image_datasets \u001b[38;5;241m=\u001b[39m {x: \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mdata_transforms\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m                   \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[1;32m     20\u001b[0m dataloaders \u001b[38;5;241m=\u001b[39m {x: torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(image_datasets[x], batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m     21\u001b[0m                                              shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     22\u001b[0m               \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[1;32m     23\u001b[0m dataset_sizes \u001b[38;5;241m=\u001b[39m {x: \u001b[38;5;28mlen\u001b[39m(image_datasets[x]) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/torchvision/datasets/folder.py:309\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    303\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    307\u001b[0m     is_valid_file: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    308\u001b[0m ):\n\u001b[0;32m--> 309\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/torchvision/datasets/folder.py:144\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    136\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m     is_valid_file: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    142\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[0;32m--> 144\u001b[0m     classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, class_to_idx, extensions, is_valid_file)\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/torchvision/datasets/folder.py:218\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m    192\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m        directory/\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/torchvision/datasets/folder.py:40\u001b[0m, in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(directory: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Finds the class folders in a dataset.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    See :class:`DatasetFolder` for details.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(entry\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_dir())\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any class folder in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'potato/train'"
     ]
    }
   ],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = 'potato'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b636737-6831-4b2a-80bc-d2cd4fb9fbe9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataloaders' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m     plt\u001b[38;5;241m.\u001b[39mpause(\u001b[38;5;241m0.001\u001b[39m)  \u001b[38;5;66;03m# pause a bit so that plots are updated\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Get a batch of training data\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m inputs, classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[43mdataloaders\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Make a grid from batch\u001b[39;00m\n\u001b[1;32m     18\u001b[0m out \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mmake_grid(inputs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloaders' is not defined"
     ]
    }
   ],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow(out, title=[class_names[x] for x in classes])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d6cb416-ac9f-40ce-af48-452c99abecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82046c8c-bf0e-44fb-ac95-225f305e7923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "431209eb-6baf-4807-910b-4f302b2f4a16",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_62774/3391283009.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel_ft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_ftrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel_ft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_ft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    895\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    896\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 897\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "model_ft = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "# Here the size of each output sample is set to 2.\n",
    "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "model_ft.fc = nn.Linear(num_ftrs, 3)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea70dcd1-e1ba-4033-9fd6-3eb18414bbfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_62774/2182967272.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n\u001b[0m\u001b[1;32m      2\u001b[0m                        num_epochs=25)\n",
      "\u001b[0;32m/tmp/ipykernel_62774/2407595539.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;31m# Iterate over data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5325cbe1-891c-4ee4-8ebf-08c22c43c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = torchvision.models.resnet18(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 3)\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opposed to before.\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5392a82f-9cfe-4781-bba7-7ce7a128d7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 0.7413 Acc: 0.6511\n",
      "val Loss: 0.3894 Acc: 0.8400\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 0.5698 Acc: 0.7644\n",
      "val Loss: 0.2268 Acc: 0.9167\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 0.5284 Acc: 0.7878\n",
      "val Loss: 0.3962 Acc: 0.8233\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 0.4190 Acc: 0.8322\n",
      "val Loss: 0.1317 Acc: 0.9633\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.4328 Acc: 0.8311\n",
      "val Loss: 0.1278 Acc: 0.9467\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.4924 Acc: 0.8233\n",
      "val Loss: 0.1266 Acc: 0.9533\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.3860 Acc: 0.8433\n",
      "val Loss: 0.1688 Acc: 0.9367\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 0.3882 Acc: 0.8467\n",
      "val Loss: 0.1538 Acc: 0.9433\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.3313 Acc: 0.8622\n",
      "val Loss: 0.0979 Acc: 0.9767\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.3154 Acc: 0.8744\n",
      "val Loss: 0.1544 Acc: 0.9400\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 0.3008 Acc: 0.8878\n",
      "val Loss: 0.1339 Acc: 0.9400\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.3737 Acc: 0.8633\n",
      "val Loss: 0.1904 Acc: 0.9167\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 0.3280 Acc: 0.8722\n",
      "val Loss: 0.2498 Acc: 0.8700\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 0.3713 Acc: 0.8689\n",
      "val Loss: 0.1248 Acc: 0.9600\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 0.3464 Acc: 0.8656\n",
      "val Loss: 0.1257 Acc: 0.9533\n",
      "\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 0.3492 Acc: 0.8622\n",
      "val Loss: 0.1378 Acc: 0.9367\n",
      "\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 0.2894 Acc: 0.8800\n",
      "val Loss: 0.1156 Acc: 0.9533\n",
      "\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 0.3573 Acc: 0.8622\n",
      "val Loss: 0.1310 Acc: 0.9467\n",
      "\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 0.3403 Acc: 0.8789\n",
      "val Loss: 0.1127 Acc: 0.9567\n",
      "\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 0.3134 Acc: 0.8711\n",
      "val Loss: 0.1810 Acc: 0.9267\n",
      "\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 0.3735 Acc: 0.8544\n",
      "val Loss: 0.1162 Acc: 0.9567\n",
      "\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 0.3311 Acc: 0.8600\n",
      "val Loss: 0.1152 Acc: 0.9700\n",
      "\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 0.3310 Acc: 0.8700\n",
      "val Loss: 0.1004 Acc: 0.9733\n",
      "\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 0.3069 Acc: 0.8811\n",
      "val Loss: 0.1792 Acc: 0.9167\n",
      "\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 0.3219 Acc: 0.8789\n",
      "val Loss: 0.1265 Acc: 0.9600\n",
      "\n",
      "Training complete in 1m 16s\n",
      "Best val Acc: 0.976667\n"
     ]
    }
   ],
   "source": [
    "model_conv = train_model(model_conv, criterion, optimizer_conv,\n",
    "                         exp_lr_scheduler, num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bddc272b-8935-4d1a-a3fa-f4b2f4d0e10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'val': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = 'gaibu'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=1,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bf70e8c-d2c3-4e59-beb0-5cf89b556004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "530"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = []\n",
    "datas = []\n",
    "for i, (data, label) in enumerate(dataloaders['val']):\n",
    "    labels.append(label.cuda())\n",
    "    datas.append(data.cuda())\n",
    "len(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa5d5fed-ae66-4f9a-9f66-52b225ac06e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.8358490566037736\n"
     ]
    }
   ],
   "source": [
    "model_conv.eval()\n",
    "passed = 0\n",
    "preds = []\n",
    "actuals = []\n",
    "for i in range(len(datas)):\n",
    "    x, y = datas[i], labels[i]\n",
    "    with torch.no_grad():\n",
    "        pred = model_conv(x)\n",
    "        predicted, actual = class_names[pred[0].argmax(0)], class_names[y]\n",
    "        preds.append(predicted)\n",
    "        actuals.append(actual)\n",
    "        #print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')\n",
    "        if predicted==actual:\n",
    "            passed += 1\n",
    "print(f\"accuracy:{passed/len(datas)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ae33398-0c0a-4516-a3a9-5ac3c67e0e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 98   2   0]\n",
      " [  0  49  18]\n",
      " [ 18  49 296]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f950cde7430>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "cm = confusion_matrix(actuals, preds)\n",
    "print(cm)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=class_names)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776e4379-153b-4b49-b224-86b2454deb3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
